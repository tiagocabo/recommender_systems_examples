{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETAILED DESCRIPTIONS OF DATA FILES\n",
    "Here are brief descriptions of the data.\n",
    "\n",
    "ml-data.tar.gz -- Compressed tar file. To rebuild the u data files do this:\n",
    "gunzip ml-data.tar.gz\n",
    "tar xvf ml-data.tar\n",
    "mku.sh\n",
    "\n",
    "**u.data** -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "Each user has rated at least 20 movies. Users and items are\n",
    "numbered consecutively from 1. The data is randomly\n",
    "ordered. This is a tab separated list of\n",
    "user id | item id | rating | timestamp.\n",
    "The time stamps are unix seconds since 1/1/1970 UTC\n",
    "\n",
    "**u.info** -- The number of users, items, and ratings in the u data set.\n",
    "\n",
    "**u.item** -- Information about the items (movies); this is a tab separated\n",
    "list of\n",
    "movie id | movie title | release date | video release date |\n",
    "IMDb URL | unknown | Action | Adventure | Animation |\n",
    "Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "Thriller | War | Western |\n",
    "The last 19 fields are the genres, a 1 indicates the movie\n",
    "is of that genre, a 0 indicates it is not; movies can be in\n",
    "several genres at once.\n",
    "The movie ids are the ones used in the u.data data set.\n",
    "\n",
    "**u.genre** -- A list of the genres.\n",
    "\n",
    "**u.user** -- Demographic information about the users; this is a tab\n",
    "separated list of\n",
    "user id | age | gender | occupation | zip code\n",
    "The user ids are the ones used in the u.data data set.\n",
    "\n",
    "**u.occupation** -- A list of the occupations.\n",
    "\n",
    "**u1.base** -- The data sets u1.base and u1.test through u5.base and u5.test\n",
    "**u1.test** are 80%/20% splits of the u data into training and test data.\n",
    "**u2.base** Each of u1, …, u5 have disjoint test sets; this if for\n",
    "**u2.test** 5 fold cross validation (where you repeat your experiment\n",
    "**u3.base with each training and test set and average the results).\n",
    "**u3.test** These data sets can be generated from u.data by mku.sh.\n",
    "**u4.base**\n",
    "**u4.test**\n",
    "**u5.base**\n",
    "**u5.test**\n",
    "\n",
    "**ua.base** -- The data sets ua.base, ua.test, ub.base, and ub.test\n",
    "**ua.test** split the u data into a training set and a test set with\n",
    "**ub.base** exactly 10 ratings per user in the test set. The sets\n",
    "**ub.test** ua.test and ub.test are disjoint. These data sets can\n",
    "be generated from u.data by mku.sh.\n",
    "\n",
    "**allbut.pl** -- The script that generates training and test sets where\n",
    "all but n of a users ratings are in the training data.\n",
    "\n",
    "**mku.sh** -- A shell script to generate all the u data sets from u.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf \n",
    "\n",
    "# Import the required ML functions\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following are the parameters of a SparkContext.\n",
    "\n",
    "**Master** − It is the URL of the cluster it connects to.\n",
    "\n",
    "**appName** − Name of your job.\n",
    "\n",
    "**sparkHome** − Spark installation directory.\n",
    "\n",
    "**pyFiles** − The .zip or .py files to send to the cluster and add to the PYTHONPATH.\n",
    "\n",
    "**Environment** − Worker nodes environment variables.\n",
    "\n",
    "**batchSize** − The number of Python objects represented as a single Java object. Set 1 to disable batching, 0 to automatically choose the batch size based on object sizes, or -1 to use an unlimited batch size.\n",
    "\n",
    "**Serializerve** − RDD serializer.\n",
    "\n",
    "**Conf** − An object of L{SparkConf} to set all the Spark properties.\n",
    "\n",
    "**Gateway** − Use an existing gateway and JVM, otherwise initializing a new JVM.\n",
    "\n",
    "**JSC** − The JavaSparkContext instance.\n",
    "**profiler_cls** − A class of custom Profiler used to do profiling (the default is pyspark.profiler.BasicProfiler).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " spark = SparkSession \\\n",
    " .builder \\\n",
    " .appName(\"recommendation_engine\") \\\n",
    " .master(\"local\") \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.csv(\"./data/ml-100k/u.data\", sep=\"\\t\")\n",
    "#ratings = spark.read.load('data/ml-100k/ratings.parquet', schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.toDF('userId','movieId','rating','timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId', 'movieId', 'rating', 'timestamp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.sort(['userId','movieId'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|     5|874965758|\n",
      "|     1|     10|     3|875693118|\n",
      "|     1|    100|     5|878543541|\n",
      "|     1|    101|     2|878542845|\n",
      "|     1|    102|     2|889751736|\n",
      "|     1|    103|     1|878542845|\n",
      "|     1|    104|     1|875241619|\n",
      "|     1|    105|     2|875240739|\n",
      "|     1|    106|     4|875241390|\n",
      "|     1|    107|     4|875241619|\n",
      "|     1|    108|     5|875240920|\n",
      "|     1|    109|     5|874965739|\n",
      "|     1|     11|     2|875072262|\n",
      "|     1|    110|     1|878542845|\n",
      "|     1|    111|     5|889751711|\n",
      "|     1|    112|     1|878542441|\n",
      "|     1|    113|     5|878542738|\n",
      "|     1|    114|     5|875072173|\n",
      "|     1|    115|     5|878541637|\n",
      "|     1|    116|     3|878542960|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show schema\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of ratings in the dataset\n",
    "numerator = ratings.select(\"rating\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique users:  943\n"
     ]
    }
   ],
   "source": [
    "# Count the number of distinct userIds and distinct movieIds\n",
    "num_users = ratings.select(\"userId\").distinct().count()\n",
    "print('number of unique users: ', num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique movies:  1682\n"
     ]
    }
   ],
   "source": [
    "num_movies = ratings.select(\"movieId\").distinct().count()\n",
    "print('number of unique movies: ', num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible ratings:  1586126\n"
     ]
    }
   ],
   "source": [
    "# Set the denominator equal to the number of users multiplied by the number of movies\n",
    "denominator = num_users * num_movies\n",
    "print('number of possible ratings: ', denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratings dataframe is  93.70% empty.\n"
     ]
    }
   ],
   "source": [
    "# Divide the numerator by the denominator\n",
    "sparsity = (1.0 - (numerator *1.0)/denominator)*100\n",
    "print(\"The ratings dataframe is \", \"%.2f\" % sparsity + \"% empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|     5|874965758|\n",
      "|     1|     10|     3|875693118|\n",
      "|     1|    100|     5|878543541|\n",
      "|     1|    101|     2|878542845|\n",
      "|     1|    102|     2|889751736|\n",
      "|     1|    103|     1|878542845|\n",
      "|     1|    104|     1|875241619|\n",
      "|     1|    105|     2|875240739|\n",
      "|     1|    106|     4|875241390|\n",
      "|     1|    107|     4|875241619|\n",
      "|     1|    108|     5|875240920|\n",
      "|     1|    109|     5|874965739|\n",
      "|     1|     11|     2|875072262|\n",
      "|     1|    110|     1|878542845|\n",
      "|     1|    111|     5|889751711|\n",
      "|     1|    112|     1|878542441|\n",
      "|     1|    113|     5|878542738|\n",
      "|     1|    114|     5|875072173|\n",
      "|     1|    115|     5|878541637|\n",
      "|     1|    116|     3|878542960|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to show only userIds less than 100\n",
    "ratings.filter(sf.col(\"userId\") < 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|userId|count|\n",
      "+------+-----+\n",
      "|   296|  147|\n",
      "|   467|   44|\n",
      "|   675|   34|\n",
      "|   691|   32|\n",
      "|   829|   64|\n",
      "|   125|  182|\n",
      "|   451|   98|\n",
      "|   800|   28|\n",
      "|   853|   41|\n",
      "|   666|  245|\n",
      "|   870|  269|\n",
      "|   919|  217|\n",
      "|   926|   20|\n",
      "|   124|   24|\n",
      "|   447|  139|\n",
      "|    51|   23|\n",
      "|   591|   84|\n",
      "|     7|  403|\n",
      "|   307|  112|\n",
      "|   475|   20|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group data by userId, count ratings\n",
    "ratings.groupBy(\"userId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie with the fewest ratings: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min num ratings for movies\n",
    "print(\"Movie with the fewest ratings: \")\n",
    "ratings.groupBy(\"movieId\").count().select(sf.min(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie with the fewest ratings: \n",
      "+-----------------+\n",
      "|       avg(count)|\n",
      "+-----------------+\n",
      "|59.45303210463734|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min num ratings for movies\n",
    "print(\"Movie with the fewest ratings: \")\n",
    "ratings.groupBy(\"movieId\").count().select(sf.avg('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with the fewest ratings: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|        20|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min num ratings for user\n",
    "print(\"User with the fewest ratings: \")\n",
    "ratings.groupBy(\"userId\").count().select(sf.min(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg num ratings per user: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|106.04453870625663|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avg num ratings per users\n",
    "print(\"Avg num ratings per user: \")\n",
    "ratings.groupBy(\"userId\").count().select(sf.avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use .printSchema() to see the datatypes of the ratings dataset\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Spark to convert the columns to the proper data types\n",
    "ratings = ratings.select(ratings.userId.cast(\"integer\"), ratings.movieId.cast(\"integer\"), ratings.rating.cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call .printSchema() again to confirm the columns are now in the correct format\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and train set\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative = True, implicitPrefs = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALS"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that a model called \"als\" was created\n",
    "type(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank,  [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.maxIter, [5, 50, 100, 200]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models to be tested:  64\n"
     ]
    }
   ],
   "source": [
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
